{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "directory = 'data/train'\n",
    "\n",
    "length_list = []\n",
    "valence_values=[]\n",
    "\n",
    "recordings = []\n",
    "\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    if filename.endswith('.pkl'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            if data['valence'] != 2.333:\n",
    "                length_list.append(len(data['audio_data']))\n",
    "                valence_values.append(data['valence'])\n",
    "                recordings.append(data['audio_data'])\n",
    "\n",
    "valence_values = np.array(valence_values)\n",
    "len(recordings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Padding to unify the length of the arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = max(len(array) for array in recordings)  # Find the maximum length\n",
    "\n",
    "# Pad each array to have the maximum length\n",
    "padded_arrays = np.array([np.pad(array, (0, max_length - len(array)), mode='constant') for array in recordings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the labels from 1 - 5 to 1-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_valence_list = sorted(list(set(valence_values)))\n",
    "\n",
    "for i in tqdm(range(len(valence_values))):\n",
    "    valence_values[i] = unique_valence_list.index(valence_values[i])\n",
    "valence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train, test, validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data and labels into training and testing sets\n",
    "X_train, X_test_help, y_train, y_test_help = train_test_split(padded_arrays, valence_values, test_size=0.4, random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_help, y_test_help, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of recordings distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.hist(length_list, bins=100)\n",
    "plt.show()\n",
    "\n",
    "print(len(length_list))\n",
    "print(len(np.unique(np.array(length_list))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valence distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valence_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m valence_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvalence_values\u001b[49m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valence_dict:\n\u001b[0;32m      5\u001b[0m         valence_dict[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valence_values' is not defined"
     ]
    }
   ],
   "source": [
    "valence_dict = {}\n",
    "\n",
    "for i in valence_values:\n",
    "    if i not in valence_dict:\n",
    "        valence_dict[i] = 1\n",
    "    else:\n",
    "        valence_dict[i] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Create tensors to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64  # You can adjust the batch size depending on your system's capability\n",
    "\n",
    "# Convert input data and labels to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)  # Use float32 for input features\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Use long if your labels are for classification tasks\n",
    "\n",
    "# Create a dataset from tensors\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "############################################################################################################\n",
    "# Repeat the same process for the test set\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "# Repeat the same process for the validation set\n",
    "X_validation_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_validation_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "validation_dataset = TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definition of MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation_function):\n",
    "        \"\"\"\n",
    "        Initialize the MLP model.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size (int): Size of the input features.\n",
    "        - hidden_sizes (list): List containing the sizes of hidden layers.\n",
    "        - output_size (int): Size of the output layer.\n",
    "        - activation_function (torch.nn.Module): Activation function for hidden layers.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        # torch.manual_seed(42)\n",
    "        torch.manual_seed(2024)\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Create hidden layers and activations dynamically\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            # Linear layer\n",
    "            self.layers.append(nn.Linear(input_size if i == 0 else hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "\n",
    "            # Activation function (except for the last layer)\n",
    "            self.layers.append(activation_function())\n",
    "\n",
    "        # Append the ouptu layer\n",
    "        self.layers.append(nn.Softmax(dim=1))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "        # Flatten the input\n",
    "        x = x.view(-1, self.input_size)\n",
    "\n",
    "        # Forward pass through hidden layers with activation functions\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    # Training Cycle\n",
    "\n",
    "def train_model(MLP_model, optimizer, num_epochs):\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.view(-1, max_length)  # Flatten the images\n",
    "            outputs = MLP_model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass\n",
    "\n",
    "            # Update weights using the step function of our custom ADAM optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store the loss. loss.item() gets the value in a tensor. This only works for scalars.\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(MLP_model, test_loader):\n",
    "    # Model Evaluation\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        MLP_model.eval()  # Set the model to evaluation mode\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(-1, max_length)  # Flatten the test images\n",
    "            outputs = MLP_model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_labels.extend(predicted.numpy())\n",
    "            true_labels.extend(labels.numpy())\n",
    "            \n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "    print(predicted_labels)\n",
    "    print(\"True Labels:\")\n",
    "    print(true_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the model and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the model\n",
    "input_size = max_length  # the longest input is 175000\n",
    "hidden_size = [64, 64]\n",
    "output_size = 17  # 10 classes for digits 0 to 9\n",
    "activation_function = nn.ReLU\n",
    "num_epochs = 10\n",
    "\n",
    "# Create the model\n",
    "model = MLP(input_size, hidden_size, output_size, activation_function)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
