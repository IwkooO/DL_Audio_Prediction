{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10557/10557 [00:26<00:00, 400.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10556"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "directory = 'data/train'\n",
    "\n",
    "length_list = []\n",
    "valence_values=[]\n",
    "\n",
    "recordings = []\n",
    "\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    if filename.endswith('.pkl'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            if data['valence'] != 2.333:\n",
    "                length_list.append(len(data['audio_data']))\n",
    "                valence_values.append(data['valence'])\n",
    "                recordings.append(data['audio_data'])\n",
    "\n",
    "len(recordings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = max(len(array) for array in recordings)  # Find the maximum length\n",
    "\n",
    "# Pad each array to have the maximum length\n",
    "padded_arrays = np.array([np.pad(array, (0, max_length - len(array)), mode='constant') for array in recordings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.25, 3.5 , 3.25, ..., 3.25, 3.75, 4.5 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valence_values = np.array(valence_values)\n",
    "valence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10556/10556 [00:00<00:00, 1681800.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1., 10.,  9., ...,  9., 11., 14.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_valence_list = sorted(list(set(valence_values)))\n",
    "\n",
    "for i in tqdm(range(len(valence_values))):\n",
    "    valence_values[i] = unique_valence_list.index(valence_values[i])\n",
    "valence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data and labels into training and testing sets\n",
    "X_train, X_test_help, y_train, y_test_help = train_test_split(padded_arrays, valence_values, test_size=0.4, random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_help, y_test_help, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.,  9., 12., ..., 15., 14., 15.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length of recordings distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10556\n",
      "5605\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.hist(length_list, bins=100)\n",
    "plt.show()\n",
    "\n",
    "print(len(length_list))\n",
    "print(len(np.unique(np.array(length_list))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valence distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0: 140,\n",
       " 10.0: 1194,\n",
       " 9.0: 1299,\n",
       " 11.0: 989,\n",
       " 4.0: 528,\n",
       " 7.0: 1031,\n",
       " 5.0: 694,\n",
       " 8.0: 1063,\n",
       " 13.0: 613,\n",
       " 0.0: 114,\n",
       " 2.0: 229,\n",
       " 3.0: 408,\n",
       " 15.0: 151,\n",
       " 12.0: 801,\n",
       " 6.0: 907,\n",
       " 16.0: 28,\n",
       " 14.0: 367}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valence_dict = {}\n",
    "\n",
    "for i in valence_values:\n",
    "    if i not in valence_dict:\n",
    "        valence_dict[i] = 1\n",
    "    else:\n",
    "        valence_dict[i] += 1\n",
    "valence_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert input data\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)  # Use float32 for input features\n",
    "\n",
    "# Convert labels\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Use long if your labels are for classification tasks\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create a dataset from tensors\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 64  # You can adjust the batch size depending on your system's capability\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  8,  9,  ..., 12,  9, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "\n",
    "### 1. Neural network with padding\n",
    "\n",
    "### 2. Neural network with 1 input 1 hidden 1 output layer\n",
    "\n",
    "### 3. Optimizer : ADAM \n",
    "\n",
    "### 4. Loss function - Categorical cross entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation_function):\n",
    "        \"\"\"\n",
    "        Initialize the MLP model.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size (int): Size of the input features.\n",
    "        - hidden_sizes (list): List containing the sizes of hidden layers.\n",
    "        - output_size (int): Size of the output layer.\n",
    "        - activation_function (torch.nn.Module): Activation function for hidden layers.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Create hidden layers and activations dynamically\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            # Linear layer\n",
    "            self.layers.append(nn.Linear(input_size if i == 0 else hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "\n",
    "            # Activation function (except for the last layer)\n",
    "            self.layers.append(activation_function())\n",
    "\n",
    "        # Append the ouptu layer\n",
    "        self.layers.append(nn.Softmax(dim=1))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "        # Flatten the input\n",
    "        x = x.view(-1, self.input_size)\n",
    "\n",
    "        # Forward pass through hidden layers with activation functions\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Cycle\n",
    "\n",
    "def train_model(MLP_model, optimizer, num_epochs):\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(-1, max_length)  # Flatten the images\n",
    "            outputs = MLP_model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass\n",
    "\n",
    "            # Update weights using the step function of our custom ADAM optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store the loss. loss.item() gets the value in a tensor. This only works for scalars.\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(MLP_model, test_loader):\n",
    "    # Model Evaluation\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        MLP_model.eval()  # Set the model to evaluation mode\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(-1, max_length)  # Flatten the test images\n",
    "            outputs = MLP_model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_labels.extend(predicted.numpy())\n",
    "            true_labels.extend(labels.numpy())\n",
    "            \n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "    print(predicted_labels)\n",
    "    print(\"True Labels:\")\n",
    "    print(true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.1547\n",
      "Epoch [2/20], Loss: 1.1126\n",
      "Epoch [3/20], Loss: 1.0642\n",
      "Epoch [4/20], Loss: 1.0203\n",
      "Epoch [5/20], Loss: 0.9868\n",
      "Epoch [6/20], Loss: 0.9644\n",
      "Epoch [7/20], Loss: 0.9533\n",
      "Epoch [8/20], Loss: 0.9466\n",
      "Epoch [9/20], Loss: 0.9430\n",
      "Epoch [10/20], Loss: 0.9404\n",
      "Epoch [11/20], Loss: 0.9394\n",
      "Epoch [12/20], Loss: 0.9387\n",
      "Epoch [13/20], Loss: 0.9382\n",
      "Epoch [14/20], Loss: 0.9383\n",
      "Epoch [15/20], Loss: 0.9377\n",
      "Epoch [16/20], Loss: 0.9375\n",
      "Epoch [17/20], Loss: 0.9373\n",
      "Epoch [18/20], Loss: 0.9372\n",
      "Epoch [19/20], Loss: 0.9369\n",
      "Epoch [20/20], Loss: 0.9373\n",
      "Test Accuracy: 70.55%\n",
      "[5, 11, 9, 9, 11, 11, 12, 9, 10, 7, 12, 10, 7, 9, 10, 11, 7, 13, 10, 1, 10, 13, 12, 10, 7, 13, 10, 9, 13, 12, 11, 13, 3, 10, 5, 7, 7, 15, 5, 11, 13, 11, 10, 12, 11, 9, 12, 4, 5, 5, 7, 9, 15, 10, 10, 12, 5, 1, 4, 0, 4, 9, 10, 9, 10, 5, 13, 4, 7, 11, 7, 12, 12, 12, 11, 0, 13, 10, 3, 11, 9, 3, 9, 5, 5, 4, 9, 15, 11, 9, 0, 7, 12, 4, 11, 11, 12, 11, 14, 15, 13, 1, 12, 11, 9, 14, 12, 5, 12, 3, 10, 10, 4, 14, 11, 11, 4, 9, 7, 9, 7, 3, 10, 11, 13, 13, 11, 10, 11, 12, 11, 9, 9, 13, 13, 7, 15, 12, 3, 10, 11, 11, 11, 9, 3, 10, 3, 7, 12, 11, 4, 5, 11, 10, 7, 7, 4, 14, 7, 10, 7, 13, 11, 12, 10, 10, 7, 9, 7, 11, 13, 11, 13, 10, 12, 11, 9, 1, 9, 11, 9, 7, 12, 14, 11, 7, 11, 1, 5, 11, 9, 10, 3, 12, 13, 5, 7, 5, 13, 9, 7, 3, 4, 4, 12, 7, 10, 5, 10, 5, 7, 10, 9, 7, 7, 11, 14, 10, 11, 11, 10, 10, 11, 4, 14, 4, 11, 11, 11, 11, 7, 12, 15, 10, 4, 9, 14, 12, 9, 13, 9, 9, 3, 7, 3, 7, 11, 9, 10, 11, 9, 10, 9, 9, 9, 9, 9, 13, 11, 9, 7, 14, 10, 11, 9, 5, 7, 9, 7, 5, 12, 7, 11, 4, 4, 10, 11, 0, 10, 11, 11, 12, 12, 7, 13, 11, 7, 14, 9, 7, 10, 13, 3, 9, 13, 5, 10, 5, 7, 7, 11, 7, 7, 7, 3, 9, 9, 14, 11, 14, 5, 11, 9, 10, 12, 12, 5, 9, 9, 4, 11, 9, 9, 10, 7, 10, 11, 10, 13, 10, 10, 5, 1, 4, 10, 11, 12, 7, 5, 13, 10, 4, 5, 7, 10, 5, 5, 10, 3, 13, 9, 11, 13, 11, 5, 3, 7, 9, 10, 3, 7, 7, 10, 10, 11, 7, 10, 11, 1, 15, 5, 9, 10, 14, 13, 5, 13, 13, 11, 11, 14, 7, 7, 11, 10, 9, 12, 9, 10, 5, 10, 9, 3, 12, 12, 1, 0, 7, 9, 10, 10, 14, 7, 7, 14, 9, 9, 10, 11, 12, 5, 3, 1, 4, 14, 9, 9, 10, 13, 10, 5, 7, 15, 14, 14, 11, 9, 3, 9, 11, 12, 10, 7, 11, 13, 11, 3, 10, 9, 1, 9, 14, 3, 3, 13, 9, 11, 12, 11, 7, 5, 14, 5, 11, 3, 9, 1, 5, 10, 5, 4, 9, 5, 9, 5, 14, 11, 9, 4, 9, 13, 12, 10, 11, 9, 9, 9, 4, 9, 10, 9, 9, 9, 14, 10, 10, 1, 10, 5, 9, 12, 14, 12, 7, 10, 12, 7, 0, 12, 11, 5, 9, 9, 3, 7, 5, 12, 12, 1, 5, 13, 5, 7, 11, 4, 7, 5, 7, 14, 7, 1, 11, 10, 0, 5, 7, 3, 13, 9, 10, 7, 7, 9, 9, 9, 7, 13, 9, 12, 9, 13, 7, 5, 10, 0, 11, 0, 13, 7, 11, 11, 9, 10, 13, 5, 12, 7, 7, 3, 11, 14, 14, 12, 3, 5, 5, 9, 9, 11, 10, 14, 15, 9, 9, 9, 5, 4, 5, 9, 12, 13, 11, 10, 11, 12, 12, 11, 12, 12, 4, 14, 11, 12, 13, 9, 10, 3, 7, 12, 10, 4, 7, 9, 9, 10, 3, 4, 13, 10, 13, 13, 7, 3, 10, 11, 12, 14, 11, 5, 5, 13, 7, 7, 7, 4, 7, 5, 3, 3, 13, 7, 10, 12, 12, 11, 5, 7, 12, 5, 7, 9, 3, 10, 9, 1, 12, 7, 9, 9, 7, 4, 7, 9, 4, 12, 13, 9, 4, 12, 9, 9, 9, 12, 9, 10, 11, 9, 12, 5, 10, 13, 11, 11, 11, 11, 5, 3, 7, 9, 10, 3, 12, 9, 10, 5, 7, 10, 11, 7, 10, 9, 7, 7, 9, 11, 10, 7, 9, 0, 12, 12, 7, 7, 12, 9, 5, 13, 7, 10, 14, 10, 7, 10, 7, 12, 9, 9, 3, 13, 9, 4, 9, 13, 7, 9, 14, 3, 9, 1, 9, 5, 5, 12, 7, 5, 9, 14, 5, 11, 9, 12, 9, 12, 10, 10, 7, 10, 9, 12, 7, 5, 5, 12, 11, 3, 9, 9, 4, 12, 1, 10, 11, 7, 12, 14, 11, 10, 1, 7, 13, 9, 10, 14, 9, 9, 11, 12, 10, 7, 10, 9, 7, 13, 12, 0, 11, 10, 9, 12, 9, 3, 9, 12, 14, 10, 11, 14, 13, 14, 11, 12, 11, 1, 3, 13, 11, 11, 9, 12, 9, 9, 5, 11, 7, 10, 9, 11, 9, 11, 1, 10, 9, 14, 10, 3, 10, 7, 5, 14, 4, 11, 5, 13, 3, 9, 11, 12, 4, 5, 9, 13, 11, 14, 11, 11, 11, 7, 1, 9, 10, 14, 13, 10, 10, 7, 13, 9, 1, 10, 5, 7, 7, 5, 10, 3, 9, 5, 12, 5, 14, 12, 12, 4, 12, 10, 10, 13, 14, 5, 10, 12, 13, 10, 9, 0, 4, 10, 13, 3, 13, 14, 9, 12, 13, 13, 13, 11, 3, 9, 12, 12, 1, 4, 12, 13, 10, 11, 7, 5, 9, 12, 3, 3, 10, 12, 10, 4, 11, 7, 7, 11, 5, 12, 12, 13, 11, 7, 7, 10, 5, 10, 9, 7, 5, 11, 10, 7, 10, 12, 9, 10, 9, 11, 12, 9, 9, 9, 13, 5, 9, 9, 7, 5, 5, 7, 9, 5, 5, 7, 3, 10, 9, 5, 12, 14, 9, 10, 1, 12, 4, 4, 10, 12, 12, 12, 5, 12, 10, 12, 11, 5, 3, 12, 13, 5, 11, 9, 4, 11, 11, 10, 9, 4, 9, 10, 12, 10, 5, 11, 7, 11, 14, 7, 5, 11, 9, 4, 9, 9, 10, 10, 5, 13, 9, 10, 10, 3, 11, 12, 7, 11, 11, 11, 11, 9, 10, 7, 4, 3, 11, 7, 11, 9, 5, 13, 4, 11, 13, 9, 9, 5, 7, 9, 7, 4, 5, 12, 10, 13, 10, 5, 9, 10, 11, 11, 7, 11, 11, 9, 9, 11, 9, 11, 9, 4, 9, 5, 3, 7, 14, 5, 10, 12, 10, 10, 9, 9, 9, 5, 4, 12, 4, 12, 9, 9, 7, 7, 3, 11, 4, 1, 4, 10, 3, 10, 9, 11, 11, 11, 4, 3, 3, 12, 5, 11, 11, 11, 7, 15, 3, 5, 7, 12, 10, 9, 10, 4, 9, 10, 5, 7, 5, 4, 9, 10, 11, 9, 10, 13, 10, 7, 12, 12, 4, 9, 10, 10, 5, 5, 4, 10, 3, 9, 9, 12, 5, 9, 10, 7, 10, 11, 9, 12, 11, 14, 14, 5, 12, 7, 10, 7, 11, 13, 10, 9, 13, 10, 10, 13, 10, 3, 7, 7, 10, 7, 11, 14, 7, 10, 5, 7, 7, 11, 13, 11, 9, 7, 11, 10, 9, 5, 4, 10, 12, 7, 14, 3, 10, 14, 5, 4, 7, 9, 10, 9, 7, 7, 3, 3, 12, 10, 3, 14, 7, 7, 7, 13, 13, 9, 10, 14, 7, 7, 4, 7, 4, 11, 4, 10, 5, 12, 5, 13, 7, 5, 4, 13, 12, 10, 12, 11, 7, 7, 12, 12, 11, 11, 11, 5, 11, 7, 11, 9, 7, 5, 9, 5, 12, 0, 11, 11, 10, 7, 10, 7, 5, 14, 13, 10, 11, 14, 3, 4, 7, 4, 7, 7, 9, 11, 10, 7, 4, 10, 10, 12, 9, 10, 12, 1, 10, 14, 10, 3, 10, 14, 11, 12, 11, 10, 12, 9, 10, 5, 4, 9, 4, 3, 11, 9, 7, 5, 10, 10, 9, 4, 3, 10, 7, 1, 13, 7, 5, 10, 5, 7, 9, 0, 10, 13, 11, 12, 12, 7, 13, 12, 4, 11, 15, 10, 15, 11, 10, 5, 14, 4, 10, 14, 11, 9, 9, 11, 10, 5, 4, 4, 5, 9, 9, 12, 12, 11, 3, 9, 9, 11, 15, 10, 11, 9, 7, 7, 0, 7, 13, 5, 14, 11, 10, 12, 7, 14, 10, 11, 3, 4, 5, 10, 3, 9, 9, 4, 5, 14, 5, 10, 11, 9, 5, 14, 11, 0, 11, 11, 7, 7, 3, 12, 7, 11, 0, 4, 5, 9, 1, 4, 13, 10, 13, 4, 11, 3, 11, 5, 9, 9, 13, 7, 13, 11, 7, 14, 9, 14, 12, 11, 12, 9, 9, 7, 7, 14, 9, 7, 9, 9, 11, 7, 11, 11, 9, 4, 11, 3, 10, 11, 9, 9, 12, 5, 7, 12, 5, 10, 14, 11, 13, 5, 4, 10, 9, 7, 5, 9, 11, 3, 7, 12, 9, 14, 3, 10, 12, 13, 12, 1, 12, 9, 5, 5, 5, 10, 10, 4, 9, 10, 10, 9, 10, 11, 5, 12, 7, 10, 7, 7, 14, 13, 13, 10, 9, 10, 9, 3, 12, 9, 11, 11, 10, 11, 7, 12, 12, 4, 14, 13, 15, 3, 10, 3, 3, 5, 10, 10, 13, 7, 10, 14, 11, 5, 14, 7, 0, 11, 7, 5, 14, 10, 12, 3, 5, 3, 12, 13, 10, 7, 14, 9, 11, 4, 11, 11, 11, 9, 3, 5, 9, 10, 11, 10, 13, 5, 14, 1, 5, 11, 5, 10, 5, 4, 10, 9, 9, 9, 9, 7, 9, 12, 12, 5, 3, 10, 5, 12, 10, 9, 11, 11, 7, 9, 10, 9, 12, 10, 9, 13, 12, 7, 7, 7, 9, 13, 5, 12, 9, 11, 10, 5, 9, 4, 9, 9, 14, 4, 4, 10, 9, 10, 11, 9, 11, 9, 11, 5, 10, 9, 11, 12, 5, 1, 13, 5, 13, 5, 11, 14, 9, 10, 1, 7, 4, 5, 9, 5, 4, 12, 11, 13, 5, 10, 12, 12, 10, 9, 5, 7, 5, 13, 14, 10, 5, 9, 11, 12, 5, 4, 9, 11, 4, 14, 7, 11, 10, 9, 9, 13, 12, 7, 9, 14, 10, 7, 13, 14, 12, 9, 9, 14, 11, 10, 11, 13, 4, 10, 11, 11, 5, 11, 9, 11, 11, 9, 9, 5, 3, 9, 0, 15, 11, 10, 9, 11, 7, 5, 5, 9, 9, 9, 12, 15, 10, 10, 7, 10, 9, 14, 9, 12, 10, 9, 14, 7, 3, 10, 11, 5, 0, 11, 9, 9, 9, 3, 13, 5, 5, 12, 10, 7, 14, 13, 11, 9, 5, 11, 7, 9, 11, 12, 10, 14, 12, 9, 9, 9, 13, 12, 7, 9, 10, 0, 9, 9, 14, 12, 12, 9, 9, 11, 14, 14, 4, 7, 5, 7, 9, 9, 7, 12, 9, 11, 13, 4, 10, 7, 9, 5, 4, 10, 7, 7, 11, 11, 3, 12, 9, 9, 3, 7, 13, 14, 9, 11, 10, 9, 9, 9, 9, 11, 10, 7, 7, 12, 9, 3, 13, 4, 12, 13, 9, 3, 11, 11, 10, 10, 7, 10, 7, 13, 12, 11, 13, 14, 9, 7, 10, 10, 13, 7, 4, 7, 9, 3, 5, 10, 12, 9, 13, 12, 7, 9, 10, 11, 12, 12, 9, 12, 9, 13, 11, 13, 4, 10, 12, 13, 7, 5, 7, 13, 5, 11, 9, 10, 9, 11, 9, 7, 7, 4, 3, 9, 10, 10, 11, 12, 9, 11, 10, 10, 5, 15, 12, 12, 7, 7, 13, 11, 10, 9, 13, 9, 11, 5, 3, 10, 10, 12, 7, 9, 12, 11, 11, 10, 14, 9, 14, 10, 7, 9, 5, 7, 11, 14, 5, 9, 5, 3, 11, 7, 3, 7, 11, 10, 11, 13, 13, 12, 9, 5, 11, 7, 13, 11, 11, 10, 11, 11, 14, 9, 13, 4, 5, 11, 5, 13, 11, 4, 9, 11, 9, 0, 11, 3, 0, 7, 9, 5, 13, 3, 5, 5, 12, 13, 7, 5, 3, 3, 11, 9, 11, 12, 10, 10, 11, 7, 7, 10, 11, 13, 3, 13, 4, 14, 9, 10, 11, 4, 9, 13, 15, 12, 7, 10, 10, 5, 11, 11, 12, 7, 5, 11, 14, 9, 4, 11, 10, 13, 13, 13, 5, 5, 12, 10, 9, 10, 10, 5, 10, 12, 9, 4, 7, 9, 11, 9, 11, 0, 5, 12, 14, 12, 14, 10, 7, 7, 3, 10, 9, 13, 11, 10, 10, 9, 9, 4, 10, 10, 5, 11, 7, 11, 12, 10, 7, 14, 7, 0, 13, 10, 12, 13, 11, 14, 10, 10, 7, 14, 5, 7, 14, 9, 10, 1, 11, 12, 9, 11, 12, 7, 3, 9, 12, 9, 10]\n",
      "True Labels:\n",
      "[8, 8, 9, 9, 16, 11, 8, 6, 10, 7, 12, 10, 7, 9, 6, 11, 7, 13, 10, 1, 10, 13, 8, 10, 8, 8, 10, 9, 13, 8, 11, 13, 3, 10, 6, 6, 7, 8, 5, 11, 13, 8, 10, 12, 11, 9, 8, 4, 5, 2, 12, 9, 15, 10, 10, 6, 5, 1, 4, 0, 4, 9, 12, 9, 10, 5, 13, 4, 2, 11, 7, 12, 9, 12, 11, 8, 6, 8, 6, 7, 9, 3, 8, 5, 8, 7, 6, 15, 11, 9, 4, 7, 2, 4, 11, 11, 12, 11, 11, 15, 13, 8, 14, 11, 9, 2, 12, 5, 12, 3, 10, 10, 4, 14, 2, 9, 8, 9, 7, 6, 7, 3, 10, 11, 13, 13, 11, 6, 11, 12, 11, 9, 8, 13, 13, 7, 15, 8, 3, 10, 11, 11, 11, 9, 8, 10, 3, 7, 12, 11, 4, 5, 11, 10, 7, 8, 4, 8, 7, 8, 7, 13, 6, 8, 6, 10, 7, 9, 7, 14, 13, 2, 2, 10, 6, 11, 9, 1, 9, 11, 8, 7, 12, 14, 8, 14, 8, 1, 5, 15, 5, 10, 3, 12, 13, 5, 7, 5, 13, 9, 7, 3, 4, 4, 12, 8, 8, 5, 6, 5, 7, 0, 9, 7, 8, 11, 14, 10, 16, 11, 6, 10, 11, 4, 14, 4, 8, 6, 11, 11, 7, 12, 15, 10, 4, 8, 6, 6, 9, 12, 9, 9, 8, 7, 3, 7, 11, 4, 10, 11, 9, 10, 8, 9, 9, 9, 9, 13, 8, 9, 8, 14, 10, 11, 14, 5, 6, 9, 8, 2, 12, 7, 11, 4, 4, 10, 11, 8, 10, 11, 14, 8, 12, 7, 13, 8, 7, 15, 6, 7, 10, 13, 12, 9, 13, 5, 10, 6, 7, 7, 11, 7, 7, 6, 3, 9, 8, 2, 11, 2, 5, 0, 9, 10, 6, 11, 8, 9, 9, 4, 11, 9, 9, 6, 6, 10, 8, 8, 13, 10, 10, 5, 1, 4, 8, 6, 12, 7, 5, 13, 10, 8, 3, 7, 10, 5, 5, 10, 8, 13, 9, 11, 13, 11, 8, 3, 6, 9, 10, 3, 7, 7, 10, 10, 11, 6, 0, 11, 1, 15, 5, 9, 10, 14, 13, 5, 13, 13, 11, 11, 8, 7, 7, 11, 6, 9, 12, 6, 10, 5, 10, 9, 3, 12, 12, 1, 0, 7, 9, 10, 10, 14, 7, 14, 14, 9, 6, 10, 11, 12, 5, 0, 6, 8, 14, 9, 9, 10, 8, 8, 5, 7, 15, 8, 14, 6, 9, 8, 9, 11, 12, 8, 7, 11, 8, 11, 3, 10, 9, 1, 9, 14, 3, 14, 13, 9, 11, 8, 6, 8, 5, 14, 5, 14, 8, 9, 8, 5, 1, 5, 6, 9, 5, 9, 8, 14, 11, 2, 4, 9, 13, 12, 10, 11, 9, 9, 8, 4, 9, 10, 6, 9, 8, 14, 10, 6, 1, 10, 5, 8, 8, 8, 7, 7, 10, 12, 7, 13, 12, 3, 11, 6, 8, 3, 7, 6, 6, 12, 6, 8, 6, 5, 7, 8, 6, 7, 4, 8, 14, 7, 2, 10, 10, 12, 5, 8, 3, 13, 9, 10, 7, 7, 9, 6, 9, 7, 13, 9, 12, 8, 13, 7, 5, 10, 0, 11, 2, 13, 7, 14, 6, 9, 10, 13, 5, 11, 8, 7, 3, 11, 14, 14, 6, 3, 5, 5, 9, 6, 11, 10, 14, 15, 8, 6, 9, 5, 8, 5, 9, 12, 13, 11, 10, 11, 6, 6, 11, 12, 12, 4, 2, 11, 6, 13, 9, 10, 3, 7, 12, 10, 4, 7, 9, 9, 10, 7, 14, 13, 10, 13, 13, 7, 3, 10, 6, 12, 14, 8, 5, 5, 8, 6, 7, 7, 4, 7, 5, 3, 3, 8, 7, 10, 12, 12, 8, 7, 7, 12, 5, 7, 9, 7, 10, 6, 6, 5, 7, 9, 8, 7, 4, 7, 9, 4, 1, 13, 9, 4, 11, 9, 9, 9, 12, 9, 10, 11, 9, 12, 5, 7, 6, 11, 6, 11, 11, 4, 8, 7, 9, 10, 3, 12, 9, 10, 8, 8, 10, 1, 6, 10, 9, 1, 2, 9, 11, 6, 7, 9, 8, 8, 12, 7, 8, 12, 9, 5, 13, 1, 10, 8, 10, 7, 10, 8, 12, 9, 9, 8, 8, 8, 4, 9, 13, 2, 9, 14, 3, 8, 1, 15, 6, 5, 4, 7, 2, 9, 8, 5, 11, 9, 12, 6, 12, 10, 6, 7, 10, 6, 12, 7, 5, 5, 8, 11, 8, 9, 9, 8, 12, 1, 10, 11, 7, 12, 14, 11, 10, 1, 7, 13, 9, 10, 14, 10, 9, 8, 12, 10, 7, 2, 9, 15, 13, 2, 0, 11, 5, 11, 12, 9, 3, 9, 12, 8, 10, 11, 14, 8, 14, 11, 12, 14, 1, 3, 8, 11, 0, 9, 12, 13, 6, 8, 11, 7, 10, 9, 6, 6, 11, 6, 10, 9, 8, 10, 3, 10, 7, 5, 6, 4, 15, 5, 8, 3, 6, 11, 12, 4, 7, 9, 13, 11, 12, 11, 11, 11, 7, 8, 6, 6, 14, 13, 10, 10, 7, 13, 8, 1, 10, 8, 7, 7, 5, 10, 3, 9, 5, 12, 5, 2, 4, 8, 4, 6, 10, 8, 13, 14, 2, 10, 12, 13, 10, 8, 0, 4, 10, 13, 3, 15, 12, 9, 12, 13, 13, 13, 11, 3, 4, 6, 12, 1, 16, 12, 13, 10, 11, 8, 5, 9, 12, 3, 3, 10, 12, 3, 4, 11, 7, 7, 11, 5, 8, 6, 13, 11, 5, 7, 10, 5, 10, 6, 7, 5, 11, 10, 7, 10, 12, 8, 10, 9, 11, 12, 8, 9, 11, 13, 5, 9, 8, 7, 5, 5, 7, 9, 5, 6, 8, 3, 10, 8, 5, 8, 8, 9, 10, 1, 12, 2, 6, 10, 8, 8, 6, 5, 8, 6, 12, 11, 5, 3, 12, 2, 5, 11, 9, 4, 8, 11, 10, 9, 4, 9, 10, 2, 10, 8, 11, 7, 6, 14, 7, 5, 11, 9, 4, 9, 9, 10, 10, 5, 13, 6, 10, 10, 8, 11, 12, 8, 8, 9, 11, 11, 6, 10, 7, 4, 4, 11, 7, 8, 8, 13, 9, 4, 11, 13, 9, 9, 5, 6, 9, 7, 6, 14, 12, 6, 8, 10, 6, 8, 10, 11, 11, 7, 11, 12, 9, 9, 12, 9, 12, 9, 4, 9, 5, 3, 6, 11, 5, 10, 8, 10, 10, 9, 9, 9, 5, 4, 12, 6, 12, 9, 9, 7, 7, 3, 8, 6, 1, 4, 10, 6, 10, 9, 11, 11, 11, 4, 3, 3, 12, 5, 11, 11, 11, 7, 15, 3, 5, 7, 12, 10, 8, 10, 8, 9, 1, 5, 7, 5, 11, 9, 10, 11, 9, 10, 3, 10, 7, 12, 12, 4, 8, 10, 10, 8, 5, 4, 10, 6, 9, 11, 12, 5, 9, 10, 7, 10, 6, 9, 6, 8, 14, 14, 5, 8, 7, 10, 7, 8, 8, 10, 9, 12, 10, 10, 13, 9, 4, 2, 12, 10, 7, 11, 14, 7, 10, 5, 7, 7, 11, 13, 11, 6, 7, 11, 10, 9, 5, 4, 10, 8, 7, 6, 3, 10, 13, 5, 4, 7, 9, 8, 9, 6, 7, 6, 3, 12, 10, 15, 8, 8, 7, 8, 13, 13, 8, 10, 14, 7, 7, 8, 7, 6, 8, 4, 10, 5, 6, 8, 13, 8, 5, 4, 13, 12, 6, 8, 8, 1, 8, 8, 12, 8, 11, 11, 11, 8, 7, 11, 9, 7, 5, 6, 5, 6, 0, 12, 11, 6, 7, 10, 7, 5, 11, 13, 10, 15, 14, 3, 4, 7, 4, 12, 7, 9, 11, 8, 7, 3, 10, 10, 12, 9, 10, 12, 1, 2, 14, 10, 3, 10, 14, 11, 12, 11, 10, 12, 9, 8, 6, 4, 9, 6, 10, 11, 2, 7, 5, 10, 10, 9, 4, 3, 10, 2, 2, 13, 7, 8, 10, 5, 7, 9, 6, 10, 13, 11, 12, 8, 7, 13, 6, 8, 11, 4, 10, 15, 6, 10, 8, 6, 1, 10, 14, 11, 14, 9, 11, 10, 5, 4, 4, 5, 9, 9, 12, 6, 11, 3, 9, 6, 11, 15, 10, 11, 9, 10, 7, 8, 7, 13, 5, 6, 11, 10, 12, 10, 14, 10, 11, 3, 4, 5, 10, 6, 9, 9, 4, 5, 12, 5, 10, 11, 9, 6, 14, 6, 0, 11, 11, 7, 7, 2, 12, 7, 11, 4, 6, 5, 6, 8, 4, 13, 8, 13, 4, 11, 3, 6, 5, 6, 9, 13, 7, 6, 11, 7, 6, 9, 14, 4, 11, 6, 8, 9, 7, 7, 0, 2, 7, 9, 15, 11, 7, 11, 11, 9, 4, 11, 0, 6, 6, 6, 9, 12, 5, 7, 12, 5, 10, 14, 11, 13, 5, 4, 10, 9, 7, 5, 9, 11, 3, 12, 8, 6, 14, 3, 8, 12, 8, 14, 1, 12, 9, 5, 5, 5, 10, 10, 4, 9, 12, 10, 9, 10, 11, 5, 12, 7, 10, 7, 7, 14, 13, 13, 10, 9, 10, 9, 3, 3, 9, 11, 11, 10, 11, 7, 2, 12, 4, 14, 6, 15, 3, 5, 6, 3, 8, 10, 10, 13, 7, 15, 15, 11, 5, 14, 3, 8, 9, 7, 5, 1, 8, 12, 3, 5, 3, 7, 13, 13, 0, 8, 9, 8, 2, 11, 11, 11, 9, 3, 5, 9, 10, 11, 10, 13, 5, 8, 1, 5, 11, 5, 2, 5, 4, 10, 9, 9, 9, 9, 2, 9, 12, 10, 5, 3, 10, 6, 6, 10, 6, 2, 4, 7, 9, 10, 9, 12, 10, 9, 8, 12, 7, 7, 7, 9, 8, 6, 12, 14, 11, 10, 6, 9, 4, 8, 9, 14, 11, 4, 10, 9, 12, 11, 9, 11, 12, 11, 5, 10, 9, 11, 12, 5, 1, 13, 5, 8, 5, 10, 8, 9, 10, 8, 7, 4, 5, 9, 5, 4, 12, 8, 13, 8, 10, 12, 12, 6, 6, 4, 7, 6, 13, 8, 6, 6, 6, 11, 12, 5, 4, 9, 11, 8, 14, 7, 8, 10, 9, 8, 13, 12, 8, 8, 8, 10, 7, 13, 14, 11, 9, 9, 1, 10, 10, 2, 8, 4, 14, 11, 11, 11, 11, 9, 8, 4, 9, 8, 5, 4, 8, 8, 15, 11, 10, 9, 11, 6, 5, 6, 9, 9, 6, 12, 8, 10, 10, 7, 10, 6, 14, 9, 8, 10, 12, 14, 7, 3, 8, 6, 8, 0, 11, 9, 9, 9, 8, 13, 10, 5, 12, 10, 7, 14, 13, 11, 6, 5, 6, 7, 6, 14, 12, 6, 6, 6, 9, 9, 9, 13, 2, 7, 9, 6, 6, 9, 6, 14, 12, 12, 9, 9, 11, 14, 15, 4, 6, 5, 7, 8, 9, 7, 2, 6, 11, 13, 4, 10, 7, 8, 5, 10, 8, 2, 7, 8, 11, 3, 12, 9, 9, 3, 7, 13, 14, 9, 11, 10, 9, 9, 6, 9, 11, 10, 7, 7, 6, 9, 3, 13, 8, 6, 13, 9, 3, 8, 11, 10, 10, 6, 14, 7, 13, 12, 6, 13, 14, 9, 3, 10, 10, 13, 7, 4, 6, 9, 3, 5, 10, 12, 9, 13, 12, 7, 9, 10, 8, 6, 6, 9, 6, 9, 3, 11, 13, 4, 6, 12, 13, 8, 8, 12, 13, 5, 9, 9, 8, 9, 11, 9, 7, 7, 1, 3, 6, 6, 10, 11, 8, 9, 11, 10, 10, 5, 15, 12, 8, 14, 7, 13, 11, 6, 9, 6, 3, 11, 5, 2, 4, 6, 12, 7, 9, 12, 11, 0, 10, 14, 2, 14, 10, 7, 2, 2, 7, 11, 14, 5, 9, 5, 6, 11, 7, 3, 7, 11, 10, 7, 13, 13, 12, 9, 5, 8, 7, 7, 11, 11, 10, 11, 11, 14, 9, 13, 4, 5, 11, 5, 13, 8, 4, 9, 11, 9, 0, 8, 6, 0, 7, 9, 5, 6, 8, 8, 14, 12, 13, 7, 5, 3, 6, 11, 9, 11, 6, 10, 6, 11, 6, 7, 10, 11, 8, 3, 13, 4, 14, 9, 10, 8, 8, 6, 13, 15, 8, 7, 10, 13, 5, 8, 8, 12, 7, 6, 11, 14, 9, 4, 6, 10, 13, 13, 13, 5, 5, 12, 6, 8, 10, 10, 5, 10, 12, 9, 4, 7, 9, 11, 9, 11, 0, 5, 12, 8, 9, 12, 10, 7, 7, 2, 10, 9, 13, 11, 10, 10, 9, 8, 4, 10, 8, 6, 11, 7, 11, 12, 0, 7, 14, 7, 6, 8, 10, 12, 13, 11, 14, 10, 4, 7, 14, 8, 8, 14, 8, 11, 1, 6, 12, 13, 11, 12, 7, 3, 9, 12, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "input_size = max_length  # MNIST images are 28x28 pixels\n",
    "hidden_size = [32, 32]\n",
    "output_size = 17  # 10 classes for digits 0 to 9\n",
    "activation_function = nn.ReLU\n",
    "num_epochs = 20\n",
    "\n",
    "# Create the model\n",
    "model = MLP(input_size, hidden_size, output_size, activation_function)\n",
    "\n",
    "# Here we create an instance of our custom_SGD class, giving as arguments the learning rate.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
